{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOSQUIMAP\n",
    "## Mapping urban malaria vector habitat suitability and human population exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Javascript code for building ToC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the working environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Python libraries and modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for setting parameters of operating system \n",
    "import os\n",
    "import sys\n",
    "# Pandas, Numpy, Scipy libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "# Module for finding pathnames matching a pattern (Unix style)\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding directory containing Python scripts that complement this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'scripts' directory to the path (directory 'scripts' contains python scripts with custom functions)\n",
    "src = os.path.abspath('../scripts')\n",
    "if src not in sys.path:\n",
    "    sys.path.append(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up environment variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please edit the script `../scripts/config_windows.py` or `../scripts/config_linux.py`containing the configuration parameters, according to your own computer setup. The following cell runs this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run ../scripts/config_windows.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions that setup the environment variables\n",
    "import environ_variables_windows as envi\n",
    "\n",
    "# Set environment variables\n",
    "envi.setup_environment_variables() \n",
    "# Display current environment variables of your computer\n",
    "# Uncomment in case of issue, it can help find what is going wrong\n",
    "#envi.print_environment_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing GRASS GIS package and module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the GRASS GIS Python scripting package\n",
    "import grass.script as gscript\n",
    "\n",
    "#Import the GRASS GIS module containing setup, initialization and clean-up functions\n",
    "import grass.script.setup as gsetup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing custom functions (from scripts in the 'scripts' directory)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function that checks if GRASS GIS database directories exist, and creates them if needed\n",
    "from grass_database import check_gisdb, check_location, check_mapset, working_mapset, launch_mapset\n",
    "\n",
    "# Function that checks if GRASS GIS add-on is installed and installs it if needed\n",
    "# Due to problems with g.extension in scripts, it is probably better to install addons using the GRASS GUI\n",
    "from gextension import check_install_addon\n",
    "\n",
    "# Function for time management\n",
    "from processing_time import start_processing, print_processing_time\n",
    "\n",
    "# Function that checks if a directory exists and creates it if needed\n",
    "from mkdirectory import check_create_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating directories if they do not exist**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure all the required input data are present in the 'indata' directories. Pairwise comparison matrices should be stored as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory 'D:\\Sabine\\mosquimap_workflow\\input_geodata' already exists\n",
      "The directory 'D:\\Sabine\\mosquimap_workflow\\input_matrices_larvae' already exists\n",
      "The directory 'D:\\Sabine\\mosquimap_workflow\\input_matrices_adults' already exists\n",
      "The directory 'D:\\Sabine\\mosquimap_workflow\\output_geodata' already exists\n",
      "The directory 'D:\\Sabine\\mosquimap_workflow\\output_matrices_analysis' already exists\n",
      "The directory 'D:\\Sabine\\mosquimap_workflow\\output_vif_analysis' already exists\n",
      "The directory 'D:\\Sabine\\mosquimap_workflow\\output_validation_data' already exists\n",
      "The directory 'D:\\Sabine\\mosquimap_workflow\\rules' already exists\n"
     ]
    }
   ],
   "source": [
    "#Check and create directories\n",
    "check_create_dir(indata['geodata'])\n",
    "check_create_dir(indata['matrices_larvae'])\n",
    "check_create_dir(indata['matrices_adults'])\n",
    "check_create_dir(outdata['outputdir'])\n",
    "check_create_dir(outdata['output_matrices_analysis'])\n",
    "check_create_dir(outdata['output_vif_analysis'])\n",
    "check_create_dir(outdata['output_validation'])\n",
    "check_create_dir(rules['rules_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check paths where Python searches for system modules\n",
    "# Uncomment in case of issue, it can help find what is going wrong\n",
    "#import sys\n",
    "#sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRASS location and mapset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Location 'DAKAR_32628' already exist\",\n",
       " \"'PERMANENT' mapset already exists in location 'DAKAR_32628'\",\n",
       " \"You are now working in mapset 'DAKAR_32628/PERMANENT'\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mapset \n",
    "launch_mapset(\"PERMANENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRASS addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r.geomorphon is already installed on your computer\n"
     ]
    }
   ],
   "source": [
    "# Check if add-on is already installed and install it if needed\n",
    "check_install_addon(\"r.geomorphon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r.vif is already installed on your computer\n"
     ]
    }
   ],
   "source": [
    "# Check if add-on is already installed and install it if needed\n",
    "check_install_addon(\"r.vif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r.texture.tiled is already installed on your computer\n"
     ]
    }
   ],
   "source": [
    "# Check if add-on is already installed and install it if needed\n",
    "check_install_addon(\"r.texture.tiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v.concave.hull is already installed on your computer\n"
     ]
    }
   ],
   "source": [
    "# Check if add-on is already installed and install it if needed\n",
    "check_install_addon(\"v.concave.hull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_importdata=start_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link the satellite image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link Pleiades image (the file is not imported, a link is created)\n",
    "#Only band 4 is necessary for the processing\n",
    "#If you don't need to view the other bands and you want to speed up processing, link only band 4\n",
    "gscript.run_command('r.external', input=indata['image'], output='PLEIADES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask out areas that are outside AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the AOI polygon and use it as a mask to limit processing to the AOI\n",
    "# The AOI polygon covers Dakar, Guediawaye, Pikine and some urban communes of Rufisque\n",
    "gscript.run_command('v.in.ogr', overwrite=True, input=indata['aoi'], output='AOI')\n",
    "gscript.run_command('r.mask', overwrite=True, vector='AOI')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import AOI for validation (smaller extent, where survey was carried out)\n",
    "#This is the area where the entomological surveys were carried out\n",
    "#It will be used as computational region for validation\n",
    "gscript.run_command('v.in.ogr', overwrite=True, input=indata['aoi4validation'], output='AOI_VAL')\n",
    "\n",
    "#Import AOI for running tests (smaller extent, set the region to this AOI \n",
    "# for faster processing when trying some function)\n",
    "gscript.run_command('v.in.ogr', overwrite=True, input=indata['aoi4tests'], output='AOI_SMALL')\n",
    "\n",
    "#Import Pleiades land cover at 0.5m resol \n",
    "#From MAUPP/REACT, with correction of obvious misclassification of airport, visual interpretation of landfill,\n",
    "#and waterbodies split into 3 classes according to size\n",
    "gscript.run_command('g.region', vector='AOI', align='PLEIADES.1', res=0.5)\n",
    "gscript.run_command('r.in.gdal', overwrite=True, input=indata['landcover'], \n",
    "                    output='LAND_COVER_ORIG')\n",
    "\n",
    "#Import Pleiades land use at 0.5m resol(from MAUPP/REACT, corrected for topology errors)\n",
    "gscript.run_command('g.region', vector='AOI', align='PLEIADES.1', res=0.5)\n",
    "gscript.run_command('r.in.gdal', overwrite=True, input=indata['landuse'], \n",
    "                    output='LU')\n",
    "\n",
    "#Import Pleiades DTM resampled to 5m resol, and mask of DTM failed pixels (produced in Geomatica)\n",
    "gscript.run_command('g.region', vector='AOI', align='PLEIADES.1', res=5)\n",
    "gscript.run_command('r.in.gdal', overwrite=True, input=indata['dtm_5m'],\n",
    "                    output='DTM')\n",
    "gscript.run_command('r.in.gdal', overwrite=True, input=indata['dtm_5m_maskfailed'],\n",
    "                    output='DTM_FAILED')\n",
    "\n",
    "#Import soil pH at 30m resol (iSDAsoil - Hengl et al., 2021)\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)\n",
    "gscript.run_command('r.in.gdal', overwrite=True, input=indata['soil_ph_30m'],\n",
    "                    output='SOIL_PH_TEMP')\n",
    "\n",
    "#Import Topographic Wetness Index produced in SAGA at 5m resol\n",
    "#SAGA TWI is better than GRASS TWI\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)\n",
    "gscript.run_command('r.in.gdal', overwrite=True, input=indata['twi'],\n",
    "                    output='TWI_TEMP')\n",
    "\n",
    "#Import population distribution modelled in MAUPP at 100m\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=100)\n",
    "gscript.run_command('r.in.gdal', overwrite=True, input=indata['pop'] ,output='POP')\n",
    "\n",
    "#Import streams (vector layer from OSM and local knowledge)\n",
    "gscript.run_command('v.in.ogr', overwrite=True, input=indata['streams'], output='STREAMS_OSM')\n",
    "\n",
    "#Import marine waters (vector layer from OSM and local knowledge)\n",
    "gscript.run_command('v.in.ogr', overwrite=True, input=indata['marine_waters'], output='MARINE_OSM')\n",
    "\n",
    "#Import the binary lc class \"waterbodies\" (output from REACT)\n",
    "gscript.run_command('g.region', vector='AOI', align='PLEIADES.1', res=0.5)\n",
    "gscript.run_command('r.in.gdal', overwrite=True, input=indata['lc_waterbodies'], \n",
    "                   output='WATERBODIES')\n",
    "\n",
    "#Import the anopheles presence points, for \"winter 2013\" (rainy season)\n",
    "#The points were sampled by UCAD in the field\n",
    "gscript.run_command('v.in.ogr', overwrite=True, input=indata['larvae_presence'], output='ANOPHELES')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data imported in 3 minutes and 9.9 seconds'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_importdata ,\"Data imported in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded population density map (elements at risk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start timer\n",
    "starttime_pop=start_processing()\n",
    "\n",
    "#Set computational region\n",
    "gscript.run_command('g.region', raster='POP', res=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-transforming and scaling the population values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POP_LOG_max = 6.89306155644823\n",
      "POP_LOG_min = 0.0\n",
      "POP_LOG = round(((POP_LOG - 0.0) / (6.89306155644823 - 0.0)*100))\n"
     ]
    }
   ],
   "source": [
    "#Input population map is already gridded, no need to aggregate\n",
    "\n",
    "#Log-transform population density\n",
    "gscript.run_command('r.mapcalc', overwrite=True, expression=\"POP1 = POP + 1 \") #1 is added to avoid errors due to 0 values\n",
    "gscript.run_command('r.mapcalc', overwrite=True, expression=\"POP_LOG = log(POP1)\")\n",
    "\n",
    "#Scale the values to [0;100] \n",
    "rastinfo = gscript.raster_info('POP_LOG')\n",
    "outmax = '%s_max' %('POP_LOG')\n",
    "rastmax = rastinfo['max']\n",
    "print(outmax, '=', rastmax ) \n",
    "outmin = '%s_min' %('POP_LOG')\n",
    "rastmin = rastinfo['min']\n",
    "print(outmin, '=', rastmin )\n",
    "\n",
    "formula = '%s = round(((%s - %s) / (%s - %s)*100))' %('POP_LOG','POP_LOG',rastmin,rastmax,rastmin)\n",
    "print(formula)\n",
    "gscript.mapcalc(formula, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate percentiles\n",
    "gscript.run_command('r.quantile', overwrite=True, input='POP_LOG', percentiles= [50, 80], flags='r', file = rules['recode_pop2classes'])\n",
    "#Recode according to percentiles\n",
    "gscript.run_command('r.recode', overwrite=True, input= 'POP_LOG', output='POP_LOG_CLASSES', rules= rules['recode_pop2classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Export map to tiff file, for preparing map layout in QGIS (if needed)\n",
    "gscript.run_command('r.out.gdal', overwrite=True, flags = 'm', input='POP_LOG_CLASSES', output= os.path.join(outdata['outputdir'],'dakar_human_population_classes_100m.tif'))\n",
    "#gscript.run_command('r.out.gdal', overwrite=True, flags = 'm', input='POP_LOG', output=  os.path.join(outdata['outputdir'],'dakar_human_population_scaled_100m.tif'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gridded population density map created in 2.1 seconds'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_pop ,\"Gridded population density map created in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded larval habitat suitability map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_criteria_larvae=start_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8 layers (factor groups) are used in the larvae habitat suitability analysis**<br />\n",
    "Land cover (categorical)<br />\n",
    "Land use (categorical)<br />\n",
    "Landforms (categorical) or relative elevation (continuous)<br />\n",
    "Soil moisture (continuous)<br />\n",
    "Soil pH (continuous)<br />\n",
    "Distance to buildings (continuous)<br />\n",
    "Distance to trees (continuous)<br />\n",
    "Water pollution (continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land cover (categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original LC layer that was imported has the following 12 classes:\n",
    "1 Low-rise buildings, 2 Swimming pools, 3 Paved surface, 4 Dumpsites, 5 Bare soil, 6 Low vegetation, 7 Trees, 8 Small waterbodies, 9 Medium waterbodies, 10 Large waterbodies, 11 Shadow, 12 Medium- and high-rise buildings <br> <br>\n",
    "The LC layer used in the larvae analysis has the following 14 classes:\n",
    "1 Buildings, 2 Swimming pools, 3 Paved surface, 4 Dumpsites, 5 Bare soil, 6 Grass, 7 Shrubs, 8 Trees, 9 Small water bodies, 10 Medium water bodies, 11 Large water bodies, 12 Water courses, 13 Marine waters, 14 Shadow <br> \n",
    "\n",
    "From the original LC layer, classes Low rise buildings and Medium- and High-rise buildings must be merged into a single class Buildings <br> \n",
    "Class Low vegetation must be split into classes Grass and Shrubs <br>\n",
    "Classes Water courses and Marine waters must be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', vector='AOI', align='PLEIADES.1', res=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning a suitable colour palette to the original LC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assign a suitable LC colour palette (for viewing, illustrations...)\n",
    "gscript.run_command('r.colors', map='LAND_COVER_ORIG', rules= rules['colors_lc_12cl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging buidings into a single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge building classes 1 and 12 into class 1\n",
    "formula = 'LAND_COVER_TEMP = (if(LAND_COVER_ORIG == 12, 1, LAND_COVER_ORIG))'\n",
    "gscript.mapcalc(formula, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting waterbodies into sub-classes according to their size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the waterbodies\n",
    "gscript.run_command('r.to.vect', overwrite=True,input='WATERBODIES', output='WATERBODIES', type = 'area')\n",
    "gscript.run_command('v.edit', overwrite=True, map='WATERBODIES', tool = 'delete', where= 'value = 0' )\n",
    "\n",
    "#Select REACT waterbodies that overlap OSM streams and convert them to raster\n",
    "gscript.run_command('v.edit', overwrite=True, map='WATERBODIES', tool = 'delete', where= 'value = 0' )\n",
    "gscript.run_command('v.select', overwrite=True, ainput='WATERBODIES', binput = 'STREAMS_OSM', output = 'STREAMS_LC', operator = \"overlap\")\n",
    "#Convert them to raster and set NULL to 0\n",
    "gscript.run_command('v.to.rast', overwrite=True, input='STREAMS_LC', output = 'STREAMS_LC', use= 'val', value = \"100\")\n",
    "gscript.run_command('r.null', overwrite=True, map='STREAMS_LC', null=0)\n",
    "\n",
    "#Select REACT waterbodies that overlap OSM marine waters and convert them to raster\n",
    "gscript.run_command('v.select', overwrite=True, ainput='WATERBODIES', binput = 'MARINE_OSM', output = 'MARINE_LC', operator = \"overlap\")\n",
    "#Convert them to raster and set NULL to 0\n",
    "gscript.run_command('v.to.rast', overwrite=True, input='MARINE_LC', output = 'MARINE_LC', use= 'val', value = \"200\")\n",
    "gscript.run_command('r.null', overwrite=True, map='MARINE_LC', null=0)\n",
    "\n",
    "#Sum the LC and the newly created layers where streams have a value between 100 and 199\n",
    "#and marine waters have a value between 200 and 299\n",
    "formula = 'LAND_COVER_TEMP2 =(LAND_COVER_TEMP + STREAMS_LC + MARINE_LC)'\n",
    "gscript.mapcalc(formula, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting low vegetation into Grass and Shrubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Warning: GLCM-based textures are computer-intensive. Expect a long processing time for the next cell.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute GLCM homogeneity (aka Inverse Difference Moment - IDM) of the NIR band of the Pleiades image, \n",
    "#in all directions, in an 11x11 window, with a lag of 1.\n",
    "#There was an error on 'intl-8.dll', so I renamed the file 'old_intl-8.dll' (C:\\Users\\ADMIN_HOME\\anaconda3\\envs\\mosquimap\\Library\\bin\\old_intl-8.dll)\n",
    "#In the same directory, there is a file named intl-8_2.dll\n",
    "gscript.run_command('r.texture.tiled', overwrite=True, input='PLEIADES.4', output='NIR', \n",
    "                    size=11, method='idm', distance='1', tile_width='1000', tile_height='1000', processes='12')\n",
    "\n",
    "#Reclass the homogeneity layer based on a threshold (for differenciating grass and shrubs)\n",
    "#lower than or equal to 0.18 is 300 (shrubs)\n",
    "#higher than 0.18 is 0 (grass)\n",
    "#The threshold was set based on visual interpretation\n",
    "formula = 'NIR_IDM_THR = (if(NIR_IDM<=0.18, 300, 0))'\n",
    "gscript.mapcalc(formula, overwrite=True)\n",
    "\n",
    "#Replace class 6 Low vegetation with 2 classes : Grass (0) and Shrubs (300)\n",
    "formula = 'LAND_COVER_TEMP3 = (if(LAND_COVER_TEMP2==6, NIR_IDM_THR, LAND_COVER_TEMP2))'\n",
    "gscript.mapcalc(formula, overwrite=True)\n",
    "\n",
    "\n",
    "#Assign a random colour palette (for viewing)\n",
    "gscript.run_command('r.colors', map='LAND_COVER_TEMP3', color='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recoding the new LC map (number the 14 classes from 1 to 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recode the new LC map\n",
    "gscript.run_command('r.recode', overwrite=True, input='LAND_COVER_TEMP3', \n",
    "                    output='LC', rules= rules['recode_lc'])\n",
    "\n",
    "#Assign a suitable LC colour palette (for viewing, illustrations...)\n",
    "gscript.run_command('r.colors', map='LC', rules=rules['colors_lc_14cl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land use (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Land use layer is used as such\n",
    "#No preprocessing required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landforms (categorical) and relative elevation (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Produce landforms and calculate mean relative elevation based on the DTM\n",
    "gscript.run_command('r.geomorphon', overwrite=True, elevation='DTM', intensity='REL_ELEVATION_TEMP', \n",
    "                    forms='LANDFORMS_TEMP', search=60, skip=0, flat=0.3, dist=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct artefacts in LANDFORMS_TEMP due to failed pixels in DTM\n",
    "#Failed pixels occur in waterbodies, so landforms value can be set to 9 (valleys) where they occur\n",
    "formula = 'LANDFORMS = (if(DTM_FAILED==0, 9, LANDFORMS_TEMP))'\n",
    "gscript.mapcalc(formula, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct artefacts in REL_ELEVATION_TEMP due to failed pixels in DTM\n",
    "#Failed pixels occur in waterbodies, so mean relative elevation value can be set to min value where they occur\n",
    "formula = 'REL_ELEVATION = (if(DTM_FAILED==0, -1, REL_ELEVATION_TEMP))'\n",
    "gscript.mapcalc(formula, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soil moisture: Topographic Wetness Index - TWI (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.35587\n"
     ]
    }
   ],
   "source": [
    "#Correct artefacts in TWI (computed in SAGA GIS) due to failed pixels in DTM\n",
    "#Failed pixels occur in waterbodies, so TWI value can be set to max where they occur\n",
    "rastinfo=gscript.raster_info('TWI_TEMP')\n",
    "rastmax = rastinfo['max']\n",
    "print(rastmax)\n",
    "formula = 'TWI = (if(DTM_FAILED==0, %s, TWI_TEMP))' %(rastmax)\n",
    "gscript.mapcalc(formula, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soil pH (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Fill holes in the pH layer\n",
    "\n",
    "#Set values 255 and 0 to null\n",
    "gscript.run_command('r.null', overwrite=True, map='SOIL_PH_TEMP', setnull='255,0')\n",
    "\n",
    "#Fill holes with rst method, and round the values\n",
    "gscript.run_command('r.fillnulls', overwrite=True, input='SOIL_PH_TEMP', output='SOIL_PH_TEMP2', method='rst')\n",
    "gscript.run_command('r.mapcalc', overwrite=True,expression=\"SOIL_PH = round( SOIL_PH_TEMP2 )\")\n",
    "\n",
    "#Assign a suitable colour palette (for viewing, illustrations...)\n",
    "gscript.run_command('r.colors', map = 'soil_ph', color = 'bgyr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance to buildings (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', raster='DTM', res=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reclassify the land cover to keep only the buildings, calculate the distance from the buildings (in m)\n",
    "gscript.run_command('r.reclass', overwrite=True, input='LC', output='BUILDINGS_TEMP', \n",
    "                    rules=rules['reclass_lc2buildings'])\n",
    "gscript.run_command('r.grow.distance', overwrite=True, input='BUILDINGS_TEMP', \n",
    "                    distance='DIST_BUILDINGS', flags = 'm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance to trees (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reclassify the land cover to keep only the trees, calculate the distance from the trees (in m)\n",
    "gscript.run_command('r.reclass', overwrite=True, input='LC', \n",
    "                    output='TREES_TEMP', rules=rules['reclass_lc2trees'])\n",
    "gscript.run_command('r.grow.distance', overwrite=True, input='TREES_TEMP', distance='DIST_TREES', flags = 'm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance to dumpsites - water pollution (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reclassify the land cover to keep only the dumpsites, calculate the distance from the dumpsites (in m)\n",
    "gscript.run_command('r.reclass', overwrite=True, input='LC', \n",
    "                    output='DUMPSITES_TEMP', rules=rules['reclass_lc2dumpsites'])\n",
    "gscript.run_command('r.grow.distance', overwrite=True, input='DUMPSITES_TEMP', \n",
    "                    distance='DIST_DUMPSITES', flags = 'm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Criteria created in 16 minutes and 24.3 seconds'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_criteria_larvae ,\"Criteria created in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing multicollinearity of criteria with VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_multicol=start_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscript.run_command('r.vif', overwrite=True, maps= ['LC', 'LU', 'REL_ELEVATION', 'TWI', 'SOIL_PH', 'DIST_BUILDINGS', 'DIST_TREES', 'DIST_DUMPSITES'], file= os.path.join(outdata['output_vif_analysis'],'VIF_8layers.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multicollinearity assessed in 3 minutes and 16.5 seconds'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_multicol ,\"Multicollinearity assessed in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:purple\">STOP ! Please check the console or the 'VIF_8layers.txt' file to ensure criteria are not correlated.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press enter to continue \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_scaling_factors=start_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AHP for scaling categorical factors (land cover, land use, land forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15 paths in the list\n",
      "Layer Weights: [0.0136 0.016  0.0343 0.0573 0.1197 0.0573 0.0499 0.0522 0.1661 0.1999\n",
      " 0.0742 0.028  0.0417 0.0899]\n",
      "Layer Weights: [0.0104 0.053  0.0337 0.0268 0.0189 0.0457 0.0309 0.0264 0.122  0.1853\n",
      " 0.2609 0.1002 0.0652 0.0206]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN_HOME\\anaconda3\\envs\\mosquimap\\lib\\site-packages\\ipykernel_launcher.py:28: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "C:\\Users\\ADMIN_HOME\\anaconda3\\envs\\mosquimap\\lib\\site-packages\\ipykernel_launcher.py:29: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Weights: [0.0164 0.025  0.0184 0.0115 0.0456 0.0312 0.0312 0.014  0.2625 0.2115\n",
      " 0.1363 0.0768 0.0988 0.0209]\n",
      "Layer Weights: [0.0213 0.0489 0.0453 0.0473 0.0427 0.0766 0.0532 0.0264 0.289  0.1678\n",
      " 0.0932 0.0156 0.022  0.0507]\n",
      "Layer Weights: [0.0368 0.0358 0.0187 0.0842 0.0648 0.072  0.0345 0.0347 0.2695 0.179\n",
      " 0.1119 0.026  0.0097 0.0224]\n",
      "Layer Weights: [0.138  0.0249 0.0297 0.0518 0.06   0.0274 0.2649 0.2523 0.085  0.0661]\n",
      "Layer Weights: [0.0658 0.013  0.0165 0.0539 0.0292 0.0304 0.3319 0.2429 0.144  0.0724]\n",
      "Layer Weights: [0.0161 0.0149 0.0186 0.0283 0.0485 0.0244 0.1594 0.15   0.2293 0.3104]\n",
      "Layer Weights: [0.0868 0.0297 0.0306 0.0334 0.0334 0.0323 0.323  0.1889 0.1328 0.1093]\n",
      "Layer Weights: [0.0684 0.0273 0.0281 0.0281 0.0242 0.0167 0.2862 0.1458 0.1991 0.1761]\n",
      "Layer Weights: [0.0261 0.1375 0.066  0.1498 0.0363 0.1126 0.1033 0.3684]\n",
      "Layer Weights: [0.0176 0.0781 0.1025 0.2228 0.0388 0.1542 0.0623 0.3237]\n",
      "Layer Weights: [0.0146 0.2381 0.043  0.186  0.0535 0.0297 0.1644 0.2706]\n",
      "Layer Weights: [0.031  0.4186 0.1564 0.1524 0.0596 0.0232 0.0768 0.082 ]\n",
      "Layer Weights: [0.0239 0.2911 0.098  0.2136 0.0228 0.0418 0.093  0.2158]\n"
     ]
    }
   ],
   "source": [
    "### Compute (1) factor weights (priority vector) and (2) consistency ratio of each pairwise matrix\n",
    "\n",
    "##COMPARE THE RELATIVE SUITABILITY OF SUBFACTORS FOR ASSIGNING A WEIGHT TO EACH OF THEM\n",
    "##ASSESS THE CONSISTENCY OF THE JUDGMENTS\n",
    "\n",
    "#Comment on the warnings (from documentation): \n",
    "#When the matrix for which the eigenvalues and eigenvectors are computed\n",
    "#is real, the resulting eigenvalues are real (0 imaginary part) or occur in conjugate pairs\n",
    "\n",
    "#Create a list of csv files with the pairwise comparison matrices (PCM), and count the csv files\n",
    "file_list = glob.glob(os.path.join(indata['matrices_larvae_intra'], '*.csv'))\n",
    "print(\"There are %s paths in the list\"%len(file_list))\n",
    "\n",
    "#Loop over the list of csv files\n",
    "for file in file_list:\n",
    "    #Read the pairwise comparison matrix (csv with headers)\n",
    "    pcm_matrix = pd.read_csv(file, sep=';')   \n",
    "    #print (pcm_matrix)\n",
    "\n",
    "    #Convert to array\n",
    "    pcm = np.array(pcm_matrix)\n",
    "    #print (pcm)\n",
    "\n",
    "    ##1-Calculate the priority vector (weight vector) of each PCM, i.e., its principal eigenvector\n",
    "    \n",
    "    eigenvalues, eigenvector=np.linalg.eig(pcm)\n",
    "    maxindex=np.argmax(eigenvalues)\n",
    "    eigenvalues=np.float32(eigenvalues) #float32\n",
    "    eigenvector=np.float32(eigenvector) #float32\n",
    "    weights=eigenvector[:, maxindex]\n",
    "\n",
    "    weights = weights/np.sum(weights)\n",
    "\n",
    "    print (\"Layer Weights: \" + str(np.around(weights,4)))\n",
    "        \n",
    "    #Create a csv file with the weights\n",
    "    filename = 'weight_vector_%s' %(os.path.basename(file))\n",
    "    path_file = os.path.join(outdata['output_matrices_analysis'], filename)\n",
    "    np.savetxt(path_file, weights, fmt='%1.4f')\n",
    "    \n",
    "    ##2-Calculate the consistency ratio (CR) of each PCM, \n",
    "    ##i.e., its consistency index (CI) versus the consistency index of a random-like array (RI) of same size\n",
    "    ##CR=CI/RI\n",
    "    ##CR<0.10 is acceptable (Saaty, 2012)\n",
    "\n",
    "    ##2.1-Calculate the Consistency Index (CI)\n",
    "\n",
    "    #Multiply the pairwise comparison matrix by the weights, in column\n",
    "    pcm_mult = np.multiply(pcm, weights)\n",
    "    #print (\"Matrix multiplied by weights (in column) =\", pcm_mult)\n",
    "\n",
    "    #Sum the lines\n",
    "    weighted_sum = np.sum(pcm_mult, axis=1)\n",
    "    #print (\"Weighted sum (in line) =\", weighted_sum)\n",
    "\n",
    "    #Mean of (weighted sums divided by weights (priority))\n",
    "    lambda_max = np.average(np.divide(weighted_sum, weights))\n",
    "    #print(\"Lambda max =\", lambda_max)\n",
    "\n",
    "    #Count number of rows in pairwise comparison matrix\n",
    "    nb_rows = pcm.shape[0]\n",
    "    #print(\"Number of rows in the matrix =\", nb_rows)\n",
    "\n",
    "    #Consistency Index (Lambda max - nb rows)/(nb_rows-1)\n",
    "    ci = ((lambda_max-nb_rows)/(nb_rows-1))\n",
    "    #print(\"Consistency Index =\", ci)\n",
    "    \n",
    "    ##2.2-Random Index (RI)\n",
    "\n",
    "    #The RI values, as a function of the matrix size, are given in (Saaty et al., 2007)\n",
    "    ri = 0.0\n",
    "    if nb_rows == 3:\n",
    "        ri = 0.52\n",
    "    elif nb_rows == 4:\n",
    "        ri = 0.89\n",
    "    elif nb_rows == 5:\n",
    "        ri = 1.11\n",
    "    elif nb_rows == 6:\n",
    "        ri = 1.25\n",
    "    elif nb_rows == 7:\n",
    "        ri = 1.35\n",
    "    elif nb_rows == 8:\n",
    "        ri = 1.40\n",
    "    elif nb_rows == 9:\n",
    "        ri = 1.45\n",
    "    elif nb_rows == 10:\n",
    "        ri = 1.49\n",
    "    elif nb_rows == 11:\n",
    "        ri = 1.52\n",
    "    elif nb_rows == 12:\n",
    "        ri = 1.54\n",
    "    elif nb_rows == 13:\n",
    "        ri = 1.56\n",
    "    elif nb_rows == 14:\n",
    "        ri = 1.58\n",
    "    elif nb_rows == 15:\n",
    "        ri = 1.59\n",
    "    #print(\"Random Index (RI) =\", ri)\n",
    "\n",
    "    ##2.3-Calculate Consistency Ratio (CR)\n",
    "    cr = (ci/ri)\n",
    "    #print(\"Consistency Ratio (CR) =\",cr)\n",
    "    \n",
    "    #Create a csv file with the consistency index, the random index, and the consistency ratio\n",
    "    filename = 'consistency_%s' %(os.path.basename(file))\n",
    "    path_file = os.path.join(outdata['output_matrices_analysis'], filename)\n",
    "    ciricr = (ci,ri,cr)\n",
    "    np.savetxt(path_file, ciricr)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute the geometric mean of weights\n",
    "\n",
    "from scipy.stats import gmean\n",
    "\n",
    "def readmycsv(args):\n",
    "    return pd.read_csv(args, header=None)\n",
    "\n",
    "##Create a list per factor\n",
    "land_cover = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'weight_vector_larvae_lc*.csv'))\n",
    "land_use = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'weight_vector_larvae_lu*.csv'))\n",
    "land_forms = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'weight_vector_larvae_lf*.csv'))\n",
    "\n",
    "##Land cover\n",
    "#Concatenate csv files and replace 0 by 1 (to avoid error when computing geometric mean)\n",
    "weights_concat = pd.concat(map(readmycsv, land_cover), axis = 1, ignore_index=True)\n",
    "weights_concat =  weights_concat.replace(0,1)\n",
    "#Compute geometric mean, scale it and export to csv file\n",
    "weights_gmean = weights_concat.apply(gmean, axis=1)\n",
    "weights_gmean = ((weights_gmean - weights_gmean.min()) / (weights_gmean.max() - weights_gmean.min()))*100\n",
    "weights_gmean.to_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_larvae_lc_geomean.csv'), header=None,index=False, float_format=\"%.0f\")\n",
    "\n",
    "##Land use\n",
    "#Concatenate csv files and replace 0 by 1 (to avoid error when computing geometric mean)\n",
    "weights_concat = pd.concat(map(readmycsv, land_use), axis = 1, ignore_index=True)\n",
    "weights_concat =  weights_concat.replace(0,1)\n",
    "#Compute geometric mean, scale it and export to csv file\n",
    "weights_gmean = weights_concat.apply(gmean, axis=1)\n",
    "weights_gmean = ((weights_gmean - weights_gmean.min()) / (weights_gmean.max() - weights_gmean.min()))*100\n",
    "weights_gmean.to_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_larvae_lu_geomean.csv'), header=None,index=False, float_format=\"%.0f\")\n",
    "\n",
    "##Land forms\n",
    "#Concatenate csv files and replace 0 by 1 (to avoid error when computing geometric mean)\n",
    "weights_concat = pd.concat(map(readmycsv, land_forms), axis = 1, ignore_index=True)\n",
    "weights_concat =  weights_concat.replace(0,1)\n",
    "#Compute geometric mean, scale it and export to csv file\n",
    "weights_gmean = weights_concat.apply(gmean, axis=1)\n",
    "weights_gmean = ((weights_gmean - weights_gmean.min()) / (weights_gmean.max() - weights_gmean.min()))*100\n",
    "weights_gmean.to_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_larvae_lf_geomean.csv'), header=None,index=False, float_format=\"%.0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Produce the categorical factor layers\n",
    "\n",
    "##LAND COVER\n",
    "\n",
    "#Create a dataframe (df) with the recoding rules\n",
    "df_larvae_lc_weights = pd.read_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_larvae_lc_geomean.csv'), sep=';', header=None)\n",
    "df_larvae_lc_weights['lc_class'] = range(1, len(df_larvae_lc_weights) + 1)\n",
    "df_larvae_lc_weights['equal'] = '='\n",
    "df_larvae_lc_weights = df_larvae_lc_weights[['lc_class', 'equal', 0]]\n",
    "#print (df_larvae_lc_weights)\n",
    "\n",
    "\n",
    "#Create a CSV file with the recoding rules (no header, no index, separator is space)\n",
    "df_larvae_lc_weights.to_csv(rules['classes_larvae_lc'], header=False, index=False, sep = ' ')\n",
    "\n",
    "#Reclassify the predictor layer\n",
    "gscript.run_command('r.reclass', overwrite=True, input='LC', output='L_LC', rules=rules['classes_larvae_lc'])\n",
    "\n",
    "\n",
    "##LAND USE\n",
    "\n",
    "#Create a dataframe (df) with the recoding rules\n",
    "df_larvae_lu_weights = pd.read_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_larvae_lu_geomean.csv'), sep=';', header=None)\n",
    "df_larvae_lu_weights['lu_class'] = range(1, len(df_larvae_lu_weights) + 1)\n",
    "df_larvae_lu_weights['equal'] = '='\n",
    "df_larvae_lu_weights = df_larvae_lu_weights[['lu_class', 'equal', 0]]\n",
    "#print (df_larvae_lu_weights)\n",
    "\n",
    "\n",
    "#Create a CSV file with the recoding rules (no header, no index, separator is space)\n",
    "df_larvae_lu_weights.to_csv(rules['classes_larvae_lu'], header=False, index=False, sep = ' ')\n",
    "\n",
    "#Reclassify the predictor layer\n",
    "gscript.run_command('r.reclass', overwrite=True, input='LU', output='L_LU', rules=rules['classes_larvae_lu'])\n",
    "\n",
    "\n",
    "##LAND FORMS\n",
    "\n",
    "#Create a dataframe (df) with the recoding rules\n",
    "df_larvae_lf_weights = pd.read_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_larvae_lf_geomean.csv'), sep=';', header=None)\n",
    "df_larvae_lf_weights['lf_class'] = range(1, len(df_larvae_lf_weights) + 1)\n",
    "df_larvae_lf_weights['equal'] = '='\n",
    "df_larvae_lf_weights = df_larvae_lf_weights[['lf_class', 'equal', 0]]\n",
    "#print (df_larvae_lf_weights)\n",
    "\n",
    "\n",
    "#Create a CSV file with the recoding rules (no header, no index, separator is space)\n",
    "df_larvae_lf_weights.to_csv(rules['classes_larvae_lf'], header=False, index=False, sep = ' ')\n",
    "\n",
    "#Reclassify the predictor layer\n",
    "gscript.run_command('r.reclass', overwrite=True, input='LANDFORMS', output='L_LF', rules=rules['classes_larvae_lf'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membership functions for scaling continuous factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region (use DTM geometry DTM, 5m*5m)\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TOPOGRAPHIC WETNESS\n",
    "\n",
    "#Rescale the topographic wetness index (TWI) to [0;100]\n",
    "gscript.run_command('r.rescale', overwrite=True, input= 'TWI', output = 'L_TW', to=[0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##SOIL pH\n",
    "\n",
    "#Rescale the soil pH to [0;100]\n",
    "gscript.run_command('r.rescale', overwrite=True, input= 'SOIL_PH', output = 'L_PH', to=[0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5555555555555556 55.55555555555556\n"
     ]
    }
   ],
   "source": [
    "##DISTANCE TO BUILDINGS\n",
    "\n",
    "\n",
    "#At a distance of 0m, we are in the buildings so suitability is 0\n",
    "#Between 0 and 10m, suitability is maximal (100)\n",
    "#Between 10 and 100m, suitability decreases with distance\n",
    "#After 100m, suitability is 0 again\n",
    "\n",
    "\n",
    "#Calculate the slope and intercept of the linear function\n",
    "#for rescaling the distance interval [10;100] to [50;0]\n",
    "from scipy.stats import linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress([10,100],[50,0])\n",
    "print(slope,intercept)\n",
    "\n",
    "#Calculate the factor value as a linear function of the distance, as y = ax + b (valid only for interval [10,100])\n",
    "formula = '%s = (%s * %s) + %s' %('TEMP',slope,'DIST_BUILDINGS',intercept)\n",
    "gscript.mapcalc(formula, overwrite=True)\n",
    "\n",
    "#Calculate the factor value\n",
    "formula = \"L_BU = round(if (DIST_BUILDINGS == 0, 0, if (DIST_BUILDINGS < 10, 100, if (DIST_BUILDINGS >= 10 && DIST_BUILDINGS < 100, TEMP, 0))))\"\n",
    "gscript.mapcalc(formula, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0101010101010102 101.01010101010101\n"
     ]
    }
   ],
   "source": [
    "##DISTANCE TO TREES\n",
    "\n",
    "#At a distance of 0m, we are in the trees so suitability is 0\n",
    "#Between 0 and 300m, suitability decreases with distance\n",
    "#After 300m, the suitability is 0 again\n",
    "\n",
    "#Calculate the slope and intercept of the linear function\n",
    "#for rescaling the distance interval [1,100] to [100, 0]\n",
    "from scipy.stats import linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress([1,100],[100,0])\n",
    "print(slope,intercept)\n",
    "\n",
    "#Calculate the factor value as a linear function of the distance, as y = ax + b (valid only for interval [1,100])\n",
    "formula = '%s = (%s * %s) + %s' %('TEMP',slope,'DIST_TREES',intercept)\n",
    "gscript.mapcalc(formula, overwrite=True)\n",
    "\n",
    "#Calculate the factor value\n",
    "formula = \"L_TR = round(if (DIST_TREES == 0, 0, if (DIST_TREES >= 1 && DIST_TREES < 100, TEMP, 0)))\"\n",
    "gscript.mapcalc(formula, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 -87.5\n"
     ]
    }
   ],
   "source": [
    "##DISTANCE TO DUMPSITES/LANDFILLS (WATER POLLUTION)\n",
    "\n",
    "#At a distance of 0m, we are in the dumpsite and suitability is not affected\n",
    "#Up to about 350m of Mbeubeuss, suitability is low due to pollution\n",
    "#From 350m to 750m, suitability increases with distance\n",
    "#After 750m, suitability is not affected\n",
    "\n",
    "\n",
    "#Calculate the slope and intercept of the linear function\n",
    "#for rescaling the distance interval [350,750] to [0, 100]\n",
    "from scipy.stats import linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress([350,750],[0,100])\n",
    "print(slope,intercept)\n",
    "\n",
    "#Calculate the factor value as a linear function of the distance, as y = ax + b (valid only for interval [1,100])\n",
    "formula = '%s = (%s * %s) + %s' %('TEMP',slope,'DIST_DUMPSITES',intercept)\n",
    "gscript.mapcalc(formula, overwrite=True)\n",
    "\n",
    "#Calculate the factor value\n",
    "formula = \"L_DU = round(if (DIST_DUMPSITES == 0, 100, if (DIST_DUMPSITES >= 1 && DIST_DUMPSITES < 350, 0, if (DIST_DUMPSITES >= 350 && DIST_DUMPSITES < 750, TEMP, 100))))\"\n",
    "gscript.mapcalc(formula, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Factors scaled in 22.9 seconds'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_scaling_factors ,\"Factors scaled in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 paths in the list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN_HOME\\anaconda3\\envs\\mosquimap\\lib\\site-packages\\ipykernel_launcher.py:25: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "C:\\Users\\ADMIN_HOME\\anaconda3\\envs\\mosquimap\\lib\\site-packages\\ipykernel_launcher.py:26: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "##COMPARE THE RELATIVE IMPORTANCE OF THE FACTORS, AND THE ASSESS THE CONSISTENCY OF THE JUDGMENTS\n",
    "\n",
    "#Comment on the warnings (from documentation): \n",
    "#When the matrix for which the eigenvalues and eigenvectors are computed\n",
    "#is real, the resulting eigenvalues are real (0 imaginary part) or occur in conjugate pairs\n",
    "\n",
    "#Create a list of csv files that contain the pairwise comparison matrices (PCM), and count the csv files\n",
    "file_list = glob.glob(os.path.join(indata['matrices_larvae_inter'], '*.csv'))\n",
    "print(\"There are %s paths in the list\"%len(file_list))\n",
    "\n",
    "#Loop over the list of csv files\n",
    "for file in file_list:\n",
    "    #Read the pairwise comparison matrix (csv with headers) into df\n",
    "    pcm_matrix = pd.read_csv(file, sep=';')   \n",
    "    #print (pcm_matrix)\n",
    "\n",
    "    #Convert into array\n",
    "    pcm = np.array(pcm_matrix)\n",
    "    #print (pcm)\n",
    "\n",
    "    ##1-Calculate the priority vector (weight vector) of each PCM, i.e., its principal eigenvector\n",
    "    \n",
    "    eigenvalues, eigenvector=np.linalg.eig(pcm)\n",
    "    maxindex=np.argmax(eigenvalues)\n",
    "    eigenvalues=np.float32(eigenvalues) #float32\n",
    "    eigenvector=np.float32(eigenvector) #float32\n",
    "    weights=eigenvector[:, maxindex]\n",
    "\n",
    "    weights = weights/np.sum(weights)\n",
    "\n",
    "    #print (\"Layer Weights: \" + str(np.around(weights,4)))\n",
    "    \n",
    "    \n",
    "    #Create a csv file with the weights\n",
    "    filename = 'weight_vector_%s' %(os.path.basename(file))\n",
    "    path_file = os.path.join(outdata['output_matrices_analysis'], filename)\n",
    "    np.savetxt(path_file, weights, fmt='%1.4f')\n",
    "    \n",
    "    ##2-Calculate the consistency ratio (CR) of each PCM, \n",
    "    ##i.e., its consistency index (CI) versus the consistency index of a random-like array (RI) of same size\n",
    "    ##CR=CI/RI\n",
    "    ##CR<0.10 is acceptable (Saaty, 2012)\n",
    "\n",
    "    ##2.1-Consistency Index (CI)\n",
    "\n",
    "    #Multiply the pairwise comparison matrix by the weights, in column\n",
    "    pcm_mult = np.multiply(pcm, weights)\n",
    "    #print (\"Matrix multiplied by weights (in column) =\", pcm_mult)\n",
    "\n",
    "    #Sum the lines\n",
    "    weighted_sum = np.sum(pcm_mult, axis=1)\n",
    "    #print (\"Weighted sum (in line) =\", weighted_sum)\n",
    "\n",
    "    #Mean of (weighted sums divided by weights (priority))\n",
    "    lambda_max = np.average(np.divide(weighted_sum, weights))\n",
    "    #print(\"Lambda max =\", lambda_max)\n",
    "\n",
    "    #Count number of rows in pairwise comparison matrix\n",
    "    nb_rows = pcm.shape[0]\n",
    "    #print(\"Number of rows in the matrix =\", nb_rows)\n",
    "\n",
    "    #Consistency Index (Lambda max - nb rows)/(nb_rows-1)\n",
    "    ci = ((lambda_max-nb_rows)/(nb_rows-1))\n",
    "    #print(\"Consistency Index =\", ci)\n",
    "    \n",
    "    ##2.2-Random Index (RI)\n",
    "\n",
    "    #The RI values, as a function of the matrix size, are given in (Saaty et al., 2007)\n",
    "    ri = 0.0\n",
    "    if nb_rows == 3:\n",
    "        ri = 0.52\n",
    "    elif nb_rows == 4:\n",
    "        ri = 0.89\n",
    "    elif nb_rows == 5:\n",
    "        ri = 1.11\n",
    "    elif nb_rows == 6:\n",
    "        ri = 1.25\n",
    "    elif nb_rows == 7:\n",
    "        ri = 1.35\n",
    "    elif nb_rows == 8:\n",
    "        ri = 1.40\n",
    "    elif nb_rows == 9:\n",
    "        ri = 1.45\n",
    "    elif nb_rows == 10:\n",
    "        ri = 1.49\n",
    "    elif nb_rows == 11:\n",
    "        ri = 1.52\n",
    "    elif nb_rows == 12:\n",
    "        ri = 1.54\n",
    "    elif nb_rows == 13:\n",
    "        ri = 1.56\n",
    "    elif nb_rows == 14:\n",
    "        ri = 1.58\n",
    "    elif nb_rows == 15:\n",
    "        ri = 1.59\n",
    "    #print(\"Random Index (RI) =\", ri)\n",
    "\n",
    "    ##2.3-Consistency Ratio (CR)\n",
    "    cr = (ci/ri)\n",
    "    #print(\"Consistency Ratio (CR) =\",cr)\n",
    "    \n",
    "    #Create a csv file with the consistency index, the random index, and the consistency ratio\n",
    "    filename = 'consistency_%s' %(os.path.basename(file))\n",
    "    path_file = os.path.join(outdata['output_matrices_analysis'], filename)\n",
    "    ciricr = (ci,ri,cr)\n",
    "    np.savetxt(path_file, ciricr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute arithmetic mean of weights\n",
    "\n",
    "def readmycsv(args):\n",
    "    return pd.read_csv(args, header=None)\n",
    "\n",
    "##Create list of csv files\n",
    "factors = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'weight_vector_larvae_factors*.csv'))\n",
    "\n",
    "#Concatenate csv files\n",
    "weights_concat = pd.concat(map(readmycsv, factors), axis = 1, ignore_index=True)\n",
    "#Compute arithmentic mean and export to csv file\n",
    "weights_mean = weights_concat.mean(axis=1)\n",
    "weights_mean.to_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_larvae_factors_mean.csv'), header=None,index=False, float_format=\"%.4f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create summary CSVs with consistency index, random index, and consistency ratio\n",
    "\n",
    "##Create lists\n",
    "land_cover = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'consistency_larvae_lc*.csv'))\n",
    "land_use = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'consistency_larvae_lu*.csv'))\n",
    "land_forms = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'consistency_larvae_lf*.csv'))\n",
    "factors = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'consistency_larvae_factors*.csv'))\n",
    "\n",
    "#Concatenate land cover consistency\n",
    "consistency_concat = pd.concat(map(readmycsv, land_cover), axis = 1, ignore_index=True)\n",
    "consistency_concat.to_csv(os.path.join(outdata['output_matrices_analysis'],\"consistency_larvae_concat_lc.csv\"), index=False, header=None, float_format=\"%.4f\")\n",
    "\n",
    "#Concatenate land use consistency\n",
    "#consistency_concat = pd.concat(map(readmycsv, land_use), axis = 1, ignore_index=True)\n",
    "consistency_concat.to_csv(os.path.join(outdata['output_matrices_analysis'], \"consistency_larvae_concat_lu.csv\"), index=False, header=None, float_format=\"%.4f\")\n",
    "\n",
    "#Concatenate land forms consistency\n",
    "#consistency_concat = pd.concat(map(readmycsv, land_forms), axis = 1, ignore_index=True)\n",
    "consistency_concat.to_csv(os.path.join(outdata['output_matrices_analysis'], \"consistency_larvae_concat_lf.csv\"), index=False, header=None, float_format=\"%.4f\")\n",
    "\n",
    "#Concatenate factors consistency\n",
    "#consistency_concat = pd.concat(map(readmycsv, factors), axis = 1, ignore_index=True)\n",
    "consistency_concat.to_csv(os.path.join(outdata['output_matrices_analysis'], \"consistency_larvae_concat_factors.csv\"), index=False, header=None, float_format=\"%.4f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:purple\">STOP ! Please check the consistency ratio and evaluate if experts should revise their judgments.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press enter to continue \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating criteria to produce the larval HSI map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_hsi=start_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0\n",
      "0  0.0680\n",
      "1  0.1239\n",
      "2  0.1386\n",
      "3  0.2466\n",
      "4  0.1240\n",
      "5  0.0513\n",
      "6  0.0419\n",
      "7  0.2057\n",
      "LARVAE_SUITABILITY_SC1_max = 86.0734\n",
      "LARVAE_SUITABILITY_SC1_min = 14.9368\n",
      "LARVAE_SUITABILITY_SC1 = round(((LARVAE_SUITABILITY_SC1 - 14.9368) / (86.0734 - 14.9368)*100))\n"
     ]
    }
   ],
   "source": [
    "#Calculate the weighted sum of factors to create the layer \"LARVAE_SUITABILITY_SC1\"\n",
    "\n",
    "df_larvae_factors_weights = pd.read_csv(os.path.join(outdata['output_matrices_analysis'], 'weight_vectors_larvae_factors_mean.csv'), header=None)\n",
    "print(df_larvae_factors_weights)\n",
    "w1 = df_larvae_factors_weights[0].values[0]\n",
    "w2 = df_larvae_factors_weights[0].values[1]\n",
    "w3 = df_larvae_factors_weights[0].values[2]\n",
    "w4 = df_larvae_factors_weights[0].values[3]\n",
    "w5 = df_larvae_factors_weights[0].values[4]\n",
    "w6 = df_larvae_factors_weights[0].values[5]\n",
    "w7 = df_larvae_factors_weights[0].values[6]\n",
    "w8 = df_larvae_factors_weights[0].values[7]\n",
    "\n",
    "formula = '%s = (%s * %s) + (%s * %s) + (%s * %s) + (%s * %s) + (%s * %s) + (%s * %s) + (%s * %s) + (%s * %s)' %('LARVAE_SUITABILITY_SC1',w1, 'L_LC', w2, 'L_LU', w3, 'L_LF', w4, 'L_TW', w5, 'L_PH', w6, 'L_BU', w7, 'L_TR', w8, 'L_DU')\n",
    "gscript.mapcalc(formula, overwrite=True)\n",
    "\n",
    "#Scale the values to [0;100] \n",
    "rastinfo = gscript.raster_info('LARVAE_SUITABILITY_SC1')\n",
    "outmax = '%s_max' %('LARVAE_SUITABILITY_SC1')\n",
    "rastmax = rastinfo['max']\n",
    "print(outmax, '=', rastmax ) \n",
    "outmin = '%s_min' %('LARVAE_SUITABILITY_SC1')\n",
    "rastmin = rastinfo['min']\n",
    "print(outmin, '=', rastmin )\n",
    "\n",
    "formula = '%s = round(((%s - %s) / (%s - %s)*100))' %('LARVAE_SUITABILITY_SC1','LARVAE_SUITABILITY_SC1',rastmin,rastmax,rastmin)\n",
    "print(formula)\n",
    "gscript.mapcalc(formula, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine with constraints\n",
    "\n",
    "\n",
    "#Use a 50m buffer inside AOI polygon to mask out beaches and rocks near seashore\n",
    "gscript.run_command('v.buffer', overwrite=True, input= \"AOI\", output=\"AOI_BUFFER50\", type=\"area\", distance=-50)\n",
    "gscript.run_command('v.to.rast', overwrite=True, input='AOI_BUFFER50', output = 'AOI_BUFFER50', use= 'val', value = \"1\")\n",
    "gscript.run_command('r.null', overwrite=True, map='AOI_BUFFER50', null=0)\n",
    "\n",
    "#Use LC classes and buffer as constraints (unsuitable areas= buildings, paved soil, trees, streams, marine waters, buffer)\n",
    "gscript.run_command('r.mapcalc', overwrite=True, \n",
    "                    expression=\"LARVAE_SUITABILITY_SC1_CONSTR = if (LC == 1 ||  LC == 3 || LC == 8 ||  LC == 12 ||  LC == 13, 0, if(AOI_BUFFER50 == 0,0, LARVAE_SUITABILITY_SC1))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the HSI map\n",
    "gscript.run_command('r.out.gdal', overwrite=True, flags = 'm', input='LARVAE_SUITABILITY_SC1_CONSTR', \n",
    "                    output=os.path.join(outdata['outputdir'],'dakar_larvae_hsi_5m_full_aoi.tif'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HSI map created in 14.4 seconds'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_hsi,\"HSI map created in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation (case where survey data are available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation : Aggregating HSI (i) in grids and (ii) in buffers around presence points (case where survey data are available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_aggrhsi=start_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region (use DTM geometry, 5m*5m)\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating HSI in fishnet and in buffers around presence points, export as CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a regular grid (fishnet) that fully covers the AOI, and calculate statistics in grid cells  \n",
    "Besides, compute the same statistics around each positive observation point (buffer same size as grid cells)  \n",
    "Export the values to 2 csv files(fishnet and buffers) for computing Continuous Boyce Index (CBI)\n",
    "and plotting HSI vs P/E in R.\n",
    "\n",
    "Repeat for multiple grid sizes is possible (e.g., 15m, 25m, 45m, 95m,...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list (if you created more than one map, add them to the list)\n",
    "raster_list = ['LARVAE_SUITABILITY_SC1_CONSTR']\n",
    "\n",
    "#Loop over cell sizes expressed in meters (if you wish to validate in multiple cell sizes, complete the list)\n",
    "#(e.g. n = np.array([15,25,35,95]))\n",
    "n = np.array([15])\n",
    "\n",
    "for x in np.nditer(n):\n",
    "    \n",
    "      \n",
    "    ##GRID##\n",
    "\n",
    "    #Generate a grid with cells of x meters * x meters (fishnet)\n",
    "    tempgrid1='GRID_'+str(x)+'_TEMP1'\n",
    "    gscript.run_command('v.mkgrid', overwrite=True, map=tempgrid1, box=[x,x])\n",
    "\n",
    "    #Drop the unnecessary columns 'row' and 'col'\n",
    "    gscript.run_command('v.db.dropcolumn', map=tempgrid1, columns='row')\n",
    "    gscript.run_command('v.db.dropcolumn', map=tempgrid1, columns='col')\n",
    "\n",
    "    #Select only (i) gridcells that fall completely within the validation AOI, AND\n",
    "    #(ii) gridcells that contain presence points but do not fall completely within the validation AOI\n",
    "    antemp='ANOPHELES_'+str(x)+'_TEMP'\n",
    "    tempgrid2='GRID_'+str(x)+'_TEMP2'\n",
    "    tempgrid3='GRID_'+str(x)+'_TEMP3'\n",
    "    tempgrid4='GRID_'+str(x)+'_TEMP4'\n",
    "    grid='GRID_'+str(x)\n",
    "    \n",
    "    gscript.run_command('v.select', overwrite=True, ainput=tempgrid1, binput='AOI_VAL', \n",
    "                        operator='within',output= tempgrid2 )\n",
    "    gscript.run_command('v.select', overwrite=True, ainput='ANOPHELES', binput=tempgrid2, \n",
    "                        operator='disjoint',output=antemp)\n",
    "    gscript.run_command('v.select', overwrite=True, ainput=tempgrid1, binput=antemp, \n",
    "                        operator='contains',output= tempgrid3)\n",
    "    gscript.run_command('v.patch', overwrite=True, flags='e', input=[tempgrid2,tempgrid3], \n",
    "                        output=tempgrid4)\n",
    "    gscript.run_command('v.clean', overwrite=True, input=tempgrid4, output=grid, \n",
    "                        tool=['rmdupl','rmdac'])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ##MANAGE ATTRIBUTES##\n",
    "    \n",
    "    #Drop the column 'presence', if it has been added to ANOPHELES during a previous run \n",
    "    gscript.run_command('v.db.dropcolumn', map='ANOPHELES', columns='presence')\n",
    "    #Add a column 'presence' to ANOPHELES and populate it with value 1\n",
    "    gscript.run_command('v.db.addcolumn', map='ANOPHELES', columns='presence integer')\n",
    "    gscript.run_command('v.db.update', map='ANOPHELES', column='presence', value='1')\n",
    "\n",
    "    \n",
    "    #Drop the columns that were added to the grid during the previous run \n",
    "    gscript.run_command('v.db.dropcolumn', map=grid, \n",
    "                        columns=['presence','suitability_average'])\n",
    "    #Add a column 'presence' to the grid and populate it with value 1 from ANOPHELES\n",
    "    #for grid cells where a there is a presence point\n",
    "    gscript.run_command('v.db.addcolumn', map=grid, columns='presence integer')\n",
    "    gscript.run_command('v.what.vect', map=grid, column='presence', query_map='ANOPHELES', \n",
    "                        query_column='presence')\n",
    "    #Replace NULL values with 0 in column 'presence'\n",
    "    gscript.run_command('v.db.update', map=grid, column='presence', value=0, \n",
    "                        where='presence IS NULL')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    ##CALCULATE STATISTICS##\n",
    "    \n",
    " \n",
    "    for raster in raster_list:\n",
    "\n",
    "        #Calculate suitability statistics in the grid (average)\n",
    "        #The computational region is AOI to avoid null values in cells not completely within AOI_VAL.\n",
    "        gscript.run_command('g.region', vector='AOI', align='DTM', res=5)\n",
    "        gscript.run_command('v.rast.stats', map=grid, raster=raster, flags='c',\n",
    "                            column_prefix='suitability', method='average')\n",
    "        gridstats='GRID_%s_'%(raster)+str(x)\n",
    "        gscript.run_command('g.copy',overwrite=True,vector=[grid,gridstats])\n",
    "               \n",
    "\n",
    "        #Calculate suitability statistics in the buffers around presence points\n",
    "        neigh= 'NEIGH_%s'%(raster)+str(x)\n",
    "        y=int(x/5) #y must be odd\n",
    "        buffstats='BUFFERS_%s_'%(raster)+str(x)\n",
    "        gscript.run_command('r.neighbors', overwrite=True, input=raster, output=neigh, \n",
    "                            method='average', size=y)\n",
    "        gscript.run_command('v.what.rast', overwrite=True, map='ANOPHELES', type='point', \n",
    "                            raster=raster, column='suitability_average')\n",
    "        gscript.run_command('g.copy',overwrite=True,vector=['ANOPHELES',buffstats])\n",
    "        gscript.run_command('v.db.dropcolumn', map='ANOPHELES', columns=['suitability_average'])\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "        ##EXPORT CSV files##\n",
    "\n",
    "        #Export the grid's attribute table as a csv file\n",
    "        gridcsv= os.path.join(outdata['output_validation'],'larvae_hsi_%s_'%(raster[19:])+str(x)+'.csv')\n",
    "        gscript.run_command('db.out.ogr', overwrite=True, input=gridstats, output=gridcsv)\n",
    "        #print(gridcsv)\n",
    "\n",
    "        #Export the buffers' attribute table as a csv file\n",
    "        buffcsv=os.path.join(outdata['output_validation'],'larvae_hsi_presence_%s_'%(raster[19:])+str(x)+'.csv')\n",
    "        gscript.run_command('db.out.ogr', overwrite=True, input=buffstats, output=buffcsv)\n",
    "        #print(buffcsv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HSI aggregated in 35 minutes and 49.2 seconds'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_aggrhsi,\"HSI aggregated in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation : Calculating CBI and plotting HSI vs P/E (R script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:purple\">STOP ! Please run the following cell in R (e.g., RStudio) and then proceed with processing in this notebook, or use R magic to call R code from this notebook.</span>\n",
    "#### <span style=\"color:purple\">Class breaks should be defined in Excel following Hirzel's method before proceeding. They should be stored in a txt file in the 'rules' directory. </span>\n",
    "Hirzel, A.H., Le Lay, G., Helfer, V., Randin, C., Guisan, A., 2006. Evaluating the ability of habitat suitability models to predict species presences. Ecological Modelling, Predicting Species Distributions 199, 142152. https://doi.org/10.1016/j.ecolmodel.2006.05.017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press enter to continue \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########R script#########\n",
    "\n",
    "#Call the ecospat library\n",
    "library(ecospat)\n",
    "\n",
    "#Set the input working directory according to your settings\n",
    "setwd(\"   \")\n",
    "\n",
    "#Import the csv file with hsi and presence data, as a dataframe\n",
    "#Note that unique() removes duplicate rows due to overlapping buffers in GRASS\n",
    "larvae_hsi <- read.csv(file = 'larvae_hsi_SC1_15.csv')\n",
    "larvae_hsi_presence <- unique(read.csv(file = 'larvae_hsi_presence_SC1_15.csv'))\n",
    "\n",
    "#Count the missing values (there should not be missing value)\n",
    "sum(is.na(larvae_hsi))\n",
    "sum(is.na(larvae_hsi_presence))\n",
    "\n",
    "#Add a column with a sequential number starting from 1\n",
    "larvae_hsi$num<-1:nrow(larvae_hsi)\n",
    "larvae_hsi_presence$num<-1:nrow(larvae_hsi_presence)\n",
    "\n",
    "\n",
    "#Check the type of each column in the dataframe\n",
    "sapply(larvae_hsi, class)\n",
    "sapply(larvae_hsi_presence, class)\n",
    "\n",
    "#Prepare the variables for calculating the Continuous Boyce Index (CBI)\n",
    "#The argument fit is a vector containing the predicted suitability values\n",
    "#The argument obs is a vector containing the predicted suitability values of the validation points (presence records)\n",
    "fit <-  larvae_hsi$suitability_average\n",
    "obs <- larvae_hsi_presence$suitability_average\n",
    "\n",
    "\n",
    "#Calculate and plot the Continuous Boyce Index (CBI)\n",
    "larvae_cbi_SC1_CONSTR_15_average <- ecospat.boyce(fit, obs, nclass = 0, window.w = 'default', res = 100, PEplot = TRUE)\n",
    "larvae_cbi_SC1_CONSTR_15_average$Spearman.cor\n",
    "\n",
    "#Set the output working directory according to your settings\n",
    "setwd(\"\")\n",
    "\n",
    "#Save the plot and optionally close the graphics device\n",
    "dev.print(svg, 'larvae_cbi_SC1_CONSTR_15_average.svg')\n",
    "#dev.off() #uncomment to close the graphics device\n",
    "\n",
    "#Round to 3 decimal digits (useful, e.g., to for use in Excel)\n",
    "round_larvae_cbi_SC1_CONSTR_15_average <- lapply(larvae_cbi_SC1_CONSTR_15_average,round,digits=3)\n",
    "\n",
    "#Print CBI and F-value (= maximum F-ratio )\n",
    "CBI <- round_larvae_cbi_SC1_CONSTR_15_average$Spearman.cor\n",
    "Fvalue <- max(round_larvae_cbi_SC1_CONSTR_15_average$F.ratio)\n",
    "\n",
    "print(paste(\"CBI = \",CBI))\n",
    "print(paste(\"F-value =\", Fvalue))\n",
    "\n",
    "#Export as csv to the 'output_validation_data' directory (set path according to your settings)\n",
    "write.csv(round_larvae_cbi_SC1_CONSTR_15_average, file = \"..\\\\larvae_cbi_SC1_CONSTR_15_average.csv\", row.names = FALSE)\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning (e.g., 4 classes: Unsuitable, Marginal, Suitable and Optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_catmap=start_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region (use DTM geometry, 5m*5m)\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res= 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate aggregated HSI (average) in a 3x3 pixel neighbourhood (15m). Resolution is still 5m but values are smoothed.\n",
    "#Reclassify HSI average based on HSI vs F-ratio curve (in our case, upper class limits: 43.0, 50.0, 75.0, 100.0)\n",
    "gscript.run_command('r.neighbors', overwrite=True, input='LARVAE_SUITABILITY_SC1_CONSTR', \n",
    "                    output='NEIGH_LARVAE_SUITABILITY_SC1_CONSTR', method='average', size=3)\n",
    "gscript.run_command('r.recode', overwrite=True, input= 'NEIGH_LARVAE_SUITABILITY_SC1_CONSTR', \n",
    "                    output='LARVAE_HABITAT_SUITABILITY_CLASSES', rules=rules['recode_larvalhsi2classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Use this cell instead if the aim is to produce a layer that is resampled to 15m\n",
    "gscript.run_command('g.region', res= 15)\n",
    "\n",
    "gscript.run_command('r.resamp.stats', overwrite=True, input='LARVAE_SUITABILITY_SC1_CONSTR', \n",
    "                    output='NEIGH_LARVAE_SUITABILITY_SC1_CONSTR', method='average')\n",
    "gscript.run_command('r.recode', overwrite=True, input= 'NEIGH_LARVAE_SUITABILITY_SC1_CONSTR', \n",
    "                    output='LARVAE_HABITAT_SUITABILITY_CLASSES', rules=rules['recode_larvalhsi2classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exporting the map to tiff file, for making map layout in QGIS\n",
    "gscript.run_command('r.out.gdal', overwrite=True, flags= 'm',input='LARVAE_HABITAT_SUITABILITY_CLASSES', \n",
    "                    output=os.path.join(outdata['outputdir'],'dakar_larvae_hs_classes_5m.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mask areas outside the population map\n",
    "gscript.run_command('r.mask', overwrite=True, raster='POP')\n",
    "\n",
    "#Export the HSI map with the restrictive mask (not beyond the population layer extent)\n",
    "#gscript.run_command('r.out.gdal', overwrite=True, flags = 'm', input='LARVAE_SUITABILITY_SC1_CONSTR', \n",
    "#                    output=os.path.join(outdata['outputdir'],'dakar_larvae_hsi_5m.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Map created in 5.9 seconds'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_catmap,\"Map created in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded adult Anopheles habitat suitability maps (hazard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing criteria: land cover, land use, distance to breeding sites, distance to buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_criteria=start_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Land cover**  \n",
    "\n",
    "The LC layer that was imported has the following 12 classes:\n",
    "1 Low-rise buildings, 2 Swimming pools, 3 Paved surface, 4 Dumpsites, 5 Bare soil, 6 Low vegetation, 7 Trees, 8 Small waterbodies, 9 Medium waterbodies, 10 Large waterbodies, 11 Shadow, 12 Medium- and high-rise buildings <br> <br>\n",
    "The LC layer used in the adult vector analysis has the following 10 classes:\n",
    "1 Low-rise buildings, 2 Medium- and high-rise buildings, 3 Swimming pools, 4 Paved surface, 5 Dumpsites, 6 Bare soil, 7 Grass, 8 Trees and shrubs, 9 Water bodies, 10 Shadow <br> \n",
    "\n",
    "Classes 1 and 12 must be merged in a single class buildings <br> \n",
    "Low vegetation must be split into Grass and Shrubs <br>\n",
    "Waterbodies must be split into isolate Water courses, Marine waters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', vector='AOI', align='PLEIADES.1', res=0.5)\n",
    "\n",
    "##LAND COVER\n",
    "\n",
    "#Merge all water bodies from the original LC in a single class\n",
    "#Order the classes as in the matrices filled out by experts\n",
    "#The resulting LC has 10 classes\n",
    "gscript.run_command('r.recode', overwrite=True, input='LAND_COVER_ORIG', output='LC_A_TEMP', rules=rules['recode_lca'])\n",
    "\n",
    "#To reclass grass, trees, shrubs, use the LC layer created for larvae suitability where they are distinct\n",
    "gscript.run_command('r.mapcalc',overwrite=True, expression='LC_A = int(if (LC == 7,8,LC_A_TEMP))')\n",
    "\n",
    "#Assign a suitable LC colour palette (for viewing, illustrations...)\n",
    "gscript.run_command('r.colors', map='LC_A', rules=rules['colors_lc_10cl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LAND USE\n",
    "\n",
    "#The LU classes are the same for larvae and adult suitability\n",
    "#No additional pre-processing required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region (use DTM geometry, 5m*5m)\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res= 5)\n",
    "\n",
    "##DISTANCE TO BREEDING SITES\n",
    "\n",
    "#Make a binary raster from the larval habitat suitability classes\n",
    "#Class 4 (optimal) = 1\n",
    "#Other classes = 0\n",
    "#This can be adapted to the context and the season\n",
    "gscript.run_command('r.mapcalc', overwrite=True, expression='L_HS_BINARY = if (LARVAE_HABITAT_SUITABILITY_CLASSES == 4, 1, 0)')\n",
    "gscript.run_command('r.null', overwrite=True, map = 'L_HS_BINARY', setnull = '0')\n",
    "\n",
    "#Calculate a distance raster from the suitable classes\n",
    "gscript.run_command('r.grow.distance', overwrite=True, flags='m', input='L_HS_BINARY', distance='L_HS_DIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Criteria produced in 4 minutes and 22.6 seconds'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_criteria,\"Criteria produced in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_scaling_factors=start_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region (use DTM geometry, 5m*5m)\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AHP for scaling categorical factors (land cover, land use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 paths in the list\n",
      "Layer Weights: [0.3347 0.1452 0.0392 0.0321 0.1129 0.0297 0.0567 0.0859 0.1026 0.061 ]\n",
      "Layer Weights: [0.2722 0.1804 0.0481 0.0129 0.0426 0.0201 0.1293 0.1419 0.0968 0.0557]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN_HOME\\anaconda3\\envs\\mosquimap\\lib\\site-packages\\ipykernel_launcher.py:28: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "C:\\Users\\ADMIN_HOME\\anaconda3\\envs\\mosquimap\\lib\\site-packages\\ipykernel_launcher.py:29: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Weights: [0.2085 0.1374 0.0274 0.0169 0.0521 0.0153 0.2379 0.2234 0.0261 0.055 ]\n",
      "Layer Weights: [0.1686 0.0237 0.1317 0.0494 0.0457 0.0457 0.0494 0.2813 0.1466 0.058 ]\n",
      "Layer Weights: [0.2287 0.1608 0.0156 0.0216 0.2581 0.0428 0.1024 0.0563 0.023  0.0906]\n",
      "Layer Weights: [0.1424 0.0314 0.0437 0.3426 0.2748 0.0761 0.0595 0.0294]\n",
      "Layer Weights: [0.0186 0.0723 0.0348 0.3331 0.0971 0.1636 0.056  0.2244]\n",
      "Layer Weights: [0.017  0.0486 0.0292 0.3607 0.244  0.1498 0.0655 0.0853]\n",
      "Layer Weights: [0.0366 0.3407 0.0871 0.2156 0.1071 0.0366 0.0924 0.0838]\n",
      "Layer Weights: [0.0267 0.2781 0.0765 0.1557 0.0291 0.0517 0.1108 0.2713]\n"
     ]
    }
   ],
   "source": [
    "### Compute (1) factor weights (priority vector) and (2) consistency ratio of each pairwise matrix\n",
    "\n",
    "##COMPARE THE RELATIVE SUITABILITY OF SUBFACTORS FOR ASSIGNING A WEIGHT TO EACH OF THEM\n",
    "##ASSESS THE CONSISTENCY OF THE JUDGMENTS\n",
    "\n",
    "#Comment on the warnings (from documentation): \n",
    "#When the matrix for which the eigenvalues and eigenvectors are computed\n",
    "#is real, the resulting eigenvalues are real (0 imaginary part) or occur in conjugate pairs\n",
    "\n",
    "#Create a list of csv files with the pairwise comparison matrices (PCM), and count the csv files\n",
    "file_list = glob.glob(os.path.join(indata['matrices_adults_intra'], '*.csv'))\n",
    "print(\"There are %s paths in the list\"%len(file_list))\n",
    "\n",
    "#Loop over the list of csv files\n",
    "for file in file_list:\n",
    "    #Read the pairwise comparison matrix (csv with headers)\n",
    "    pcm_matrix = pd.read_csv(file, sep=';')   \n",
    "    #print (pcm_matrix)\n",
    "\n",
    "    #Convert to array\n",
    "    pcm = np.array(pcm_matrix)\n",
    "    #print (pcm)\n",
    "\n",
    "    ##1-Calculate the priority vector (weight vector) of each PCM, i.e., its principal eigenvector\n",
    "    \n",
    "    eigenvalues, eigenvector=np.linalg.eig(pcm)\n",
    "    maxindex=np.argmax(eigenvalues)\n",
    "    eigenvalues=np.float32(eigenvalues) #float32\n",
    "    eigenvector=np.float32(eigenvector) #float32\n",
    "    weights=eigenvector[:, maxindex]\n",
    "\n",
    "    weights = weights/np.sum(weights)\n",
    "\n",
    "    print (\"Layer Weights: \" + str(np.around(weights,4)))\n",
    "        \n",
    "    #Create a csv file with the weights\n",
    "    filename = 'weight_vector_%s' %(os.path.basename(file))\n",
    "    path_file = os.path.join(outdata['output_matrices_analysis'], filename)\n",
    "    np.savetxt(path_file, weights, fmt='%1.4f')\n",
    "    \n",
    "    ##2-Calculate the consistency ratio (CR) of each PCM, \n",
    "    ##i.e., its consistency index (CI) versus the consistency index of a random-like array (RI) of same size\n",
    "    ##CR=CI/RI\n",
    "    ##CR<0.10 is acceptable (Saaty, 2012)\n",
    "\n",
    "    ##2.1-Calculate the Consistency Index (CI)\n",
    "\n",
    "    #Multiply the pairwise comparison matrix by the weights, in column\n",
    "    pcm_mult = np.multiply(pcm, weights)\n",
    "    #print (\"Matrix multiplied by weights (in column) =\", pcm_mult)\n",
    "\n",
    "    #Sum the lines\n",
    "    weighted_sum = np.sum(pcm_mult, axis=1)\n",
    "    #print (\"Weighted sum (in line) =\", weighted_sum)\n",
    "\n",
    "    #Mean of (weighted sums divided by weights (priority))\n",
    "    lambda_max = np.average(np.divide(weighted_sum, weights))\n",
    "    #print(\"Lambda max =\", lambda_max)\n",
    "\n",
    "    #Count number of rows in pairwise comparison matrix\n",
    "    nb_rows = pcm.shape[0]\n",
    "    #print(\"Number of rows in the matrix =\", nb_rows)\n",
    "\n",
    "    #Consistency Index (Lambda max - nb rows)/(nb_rows-1)\n",
    "    ci = ((lambda_max-nb_rows)/(nb_rows-1))\n",
    "    #print(\"Consistency Index =\", ci)\n",
    "    \n",
    "    ##2.2-Random Index (RI)\n",
    "\n",
    "    #The RI values, as a function of the matrix size, are given in (Saaty et al., 2007)\n",
    "    ri = 0.0\n",
    "    if nb_rows == 3:\n",
    "        ri = 0.52\n",
    "    elif nb_rows == 4:\n",
    "        ri = 0.89\n",
    "    elif nb_rows == 5:\n",
    "        ri = 1.11\n",
    "    elif nb_rows == 6:\n",
    "        ri = 1.25\n",
    "    elif nb_rows == 7:\n",
    "        ri = 1.35\n",
    "    elif nb_rows == 8:\n",
    "        ri = 1.40\n",
    "    elif nb_rows == 9:\n",
    "        ri = 1.45\n",
    "    elif nb_rows == 10:\n",
    "        ri = 1.49\n",
    "    elif nb_rows == 11:\n",
    "        ri = 1.52\n",
    "    elif nb_rows == 12:\n",
    "        ri = 1.54\n",
    "    elif nb_rows == 13:\n",
    "        ri = 1.56\n",
    "    elif nb_rows == 14:\n",
    "        ri = 1.58\n",
    "    elif nb_rows == 15:\n",
    "        ri = 1.59\n",
    "    #print(\"Random Index (RI) =\", ri)\n",
    "\n",
    "    ##2.3-Calculate Consistency Ratio (CR)\n",
    "    cr = (ci/ri)\n",
    "    #print(\"Consistency Ratio (CR) =\",cr)\n",
    "    \n",
    "    #Create a csv file with the consistency index, the random index, and the consistency ratio\n",
    "    filename = 'consistency_%s' %(os.path.basename(file))\n",
    "    path_file = os.path.join(outdata['output_matrices_analysis'], filename)\n",
    "    ciricr = (ci,ri,cr)\n",
    "    np.savetxt(path_file, ciricr)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute the geometric mean of weights\n",
    "\n",
    "from scipy.stats import gmean\n",
    "\n",
    "def readmycsv(args):\n",
    "    return pd.read_csv(args, header=None)\n",
    "\n",
    "##Create a list per factor\n",
    "land_cover = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'weight_vector_adults_lc*.csv'))\n",
    "land_use = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'weight_vector_adults_lu*.csv'))\n",
    "\n",
    "##Land cover\n",
    "#Concatenate csv files and replace 0 by 1 (to avoid error when computing geometric mean)\n",
    "weights_concat = pd.concat(map(readmycsv, land_cover), axis = 1, ignore_index=True)\n",
    "weights_concat =  weights_concat.replace(0,1)\n",
    "#Compute geometric mean, scale it and export to csv file\n",
    "weights_gmean = weights_concat.apply(gmean, axis=1)\n",
    "weights_gmean = ((weights_gmean - weights_gmean.min()) / (weights_gmean.max() - weights_gmean.min()))*100\n",
    "weights_gmean.to_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_adults_lc_geomean.csv'), header=None,index=False, float_format=\"%.0f\")\n",
    "\n",
    "##Land use\n",
    "#Concatenate csv files and replace 0 by 1 (to avoid error when computing geometric mean)\n",
    "weights_concat = pd.concat(map(readmycsv, land_use), axis = 1, ignore_index=True)\n",
    "weights_concat =  weights_concat.replace(0,1)\n",
    "#Compute geometric mean, scale it and export to csv file\n",
    "weights_gmean = weights_concat.apply(gmean, axis=1)\n",
    "weights_gmean = ((weights_gmean - weights_gmean.min()) / (weights_gmean.max() - weights_gmean.min()))*100\n",
    "weights_gmean.to_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_adults_lu_geomean.csv'), header=None,index=False, float_format=\"%.0f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Produce the categorical factor layers\n",
    "\n",
    "##LAND COVER\n",
    "\n",
    "#Create a dataframe (df) with the recoding rules\n",
    "mycsv = os.path.join(outdata['output_matrices_analysis'], 'weight_vectors_adults_lc_geomean.csv')\n",
    "df_adults_lc_weights = pd.read_csv(mycsv, sep=';', header=None)\n",
    "df_adults_lc_weights['lc_class'] = range(1, len(df_adults_lc_weights) + 1)\n",
    "df_adults_lc_weights['equal'] = '='\n",
    "df_adults_lc_weights = df_adults_lc_weights[['lc_class', 'equal', 0]]\n",
    "#print (df_adults_lc_weights)\n",
    "\n",
    "#Create a CSV file with the recoding rules (no header, no index, separator is space)\n",
    "df_adults_lc_weights.to_csv(rules['classes_adults_lc'], header=False, index=False, sep = ' ')\n",
    "\n",
    "#Reclassify the predictor layer\n",
    "gscript.run_command('r.reclass', overwrite=True, input='LC_A', output='A_LC', rules=rules['classes_adults_lc'])\n",
    "\n",
    "\n",
    "##LAND USE\n",
    "\n",
    "#Create a dataframe (df) with the recoding rules\n",
    "mycsv = os.path.join(outdata['output_matrices_analysis'], 'weight_vectors_adults_lu_geomean.csv')\n",
    "df_adults_lu_weights = pd.read_csv(mycsv, sep=';', header=None)\n",
    "df_adults_lu_weights['lu_class'] = range(1, len(df_adults_lu_weights) + 1)\n",
    "df_adults_lu_weights['equal'] = '='\n",
    "df_adults_lu_weights = df_adults_lu_weights[['lu_class', 'equal', 0]]\n",
    "#print (df_adults_lu_weights)\n",
    "\n",
    "#Create a CSV file with the recoding rules (no header, no index, separator is space)\n",
    "df_adults_lu_weights.to_csv(rules['classes_adults_lu'], header=False, index=False, sep = ' ')\n",
    "\n",
    "#Reclassify the predictor layer\n",
    "gscript.run_command('r.reclass', overwrite=True, input='LU', output='A_LU', rules=rules['classes_adults_lu'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membership function for scaling continuous factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_DISPERSAL_max = 100.0\n",
      "A_DISPERSAL_min = 0.0\n",
      "A_DISPERSAL = round(((A_DISPERSAL - 0.0) / (100.0 - 0.0)*100))\n"
     ]
    }
   ],
   "source": [
    "##DISPERSAL (SPREAD) FROM OPTIMAL BREEDING SITES\n",
    "\n",
    "#Apply a function that reflects the dispersal range of adult Anopheles and normalise it to [0;100]\n",
    "#The dispersal range \n",
    "gscript.run_command('r.mapcalc', overwrite=True,expression='A_DISPERSAL = if (L_HS_DIST == 0, 100, (if (L_HS_DIST > 0 && L_HS_DIST < 410, ((-0.0002505712*(L_HS_DIST^2)) + (-0.1403726401*(L_HS_DIST)) + 100), 0)))')\n",
    "\n",
    "#Scale the values to [0;100] \n",
    "rastinfo = gscript.raster_info('A_DISPERSAL')\n",
    "outmax = '%s_max' %('A_DISPERSAL')\n",
    "rastmax = rastinfo['max']\n",
    "print(outmax, '=', rastmax ) \n",
    "outmin = '%s_min' %('A_DISPERSAL')\n",
    "rastmin = rastinfo['min']\n",
    "print(outmin, '=', rastmin )\n",
    "\n",
    "formula = '%s = round(((%s - %s) / (%s - %s)*100))' %('A_DISPERSAL','A_DISPERSAL',rastmin,rastmax,rastmin)\n",
    "print(formula)\n",
    "gscript.mapcalc(formula, overwrite=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.6315789473684212 52.631578947368425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##DISTANCE TO BUILDINGS\n",
    "\n",
    "\n",
    "#At a distance of 0m, we are in the buildings so suitability is maximal (100)\n",
    "#Between 1 and 20m, suitability decreases with distance from 50 to 0\n",
    "#After 100m, suitability is 0 again\n",
    "\n",
    "\n",
    "#Calculate the slope and intercept of the linear function\n",
    "#for rescaling the distance interval [1;20] to [50;0]\n",
    "from scipy.stats import linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress([1,20],[50,0])\n",
    "print(slope,intercept)\n",
    "\n",
    "#Calculate the factor value as a linear function of the distance, as y = ax + b (valid only for interval [10,100])\n",
    "formula = '%s = (%s * %s) + %s' %('TEMP',slope,'DIST_BUILDINGS',intercept)\n",
    "gscript.mapcalc(formula, overwrite=True)\n",
    "\n",
    "#Calculate the factor value\n",
    "gscript.run_command('r.mapcalc', overwrite=True,expression = \"A_BU = round(if (DIST_BUILDINGS == 0, 100, if (DIST_BUILDINGS > 0 && DIST_BUILDINGS < 20, TEMP, 0)))\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Factors scaled in 16.3 seconds'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_scaling_factors,\"Factors scaled in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 paths in the list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN_HOME\\anaconda3\\envs\\mosquimap\\lib\\site-packages\\ipykernel_launcher.py:25: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "C:\\Users\\ADMIN_HOME\\anaconda3\\envs\\mosquimap\\lib\\site-packages\\ipykernel_launcher.py:26: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "##COMPARE THE RELATIVE IMPORTANCE OF THE FACTORS, AND THE ASSESS THE CONSISTENCY OF THE JUDGMENTS\n",
    "\n",
    "#Comment on the warnings (from documentation): \n",
    "#When the matrix for which the eigenvalues and eigenvectors are computed\n",
    "#is real, the resulting eigenvalues are real (0 imaginary part) or occur in conjugate pairs\n",
    "\n",
    "#Create a list of csv files that contain the pairwise comparison matrices (PCM), and count the csv files\n",
    "file_list = glob.glob(os.path.join(indata['matrices_adults_inter'], '*.csv'))\n",
    "print(\"There are %s paths in the list\"%len(file_list))\n",
    "\n",
    "#Loop over the list of csv files\n",
    "for file in file_list:\n",
    "    #Read the pairwise comparison matrix (csv with headers) into df\n",
    "    pcm_matrix = pd.read_csv(file, sep=';')   \n",
    "    #print (pcm_matrix)\n",
    "\n",
    "    #Convert into array\n",
    "    pcm = np.array(pcm_matrix)\n",
    "    #print (pcm)\n",
    "\n",
    "    ##1-Calculate the priority vector (weight vector) of each PCM, i.e., its principal eigenvector\n",
    "    \n",
    "    eigenvalues, eigenvector=np.linalg.eig(pcm)\n",
    "    maxindex=np.argmax(eigenvalues)\n",
    "    eigenvalues=np.float32(eigenvalues) #float32\n",
    "    eigenvector=np.float32(eigenvector) #float32\n",
    "    weights=eigenvector[:, maxindex]\n",
    "\n",
    "    weights = weights/np.sum(weights)\n",
    "\n",
    "    #print (\"Layer Weights: \" + str(np.around(weights,4)))\n",
    "    \n",
    "    \n",
    "    #Create a csv file with the weights\n",
    "    filename = 'weight_vector_%s' %(os.path.basename(file))\n",
    "    path_file = os.path.join(outdata['output_matrices_analysis'], filename)\n",
    "    np.savetxt(path_file, weights, fmt='%1.4f')\n",
    "    \n",
    "    ##2-Calculate the consistency ratio (CR) of each PCM, \n",
    "    ##i.e., its consistency index (CI) versus the consistency index of a random-like array (RI) of same size\n",
    "    ##CR=CI/RI\n",
    "    ##CR<0.10 is acceptable (Saaty, 2012)\n",
    "\n",
    "    ##2.1-Consistency Index (CI)\n",
    "\n",
    "    #Multiply the pairwise comparison matrix by the weights, in column\n",
    "    pcm_mult = np.multiply(pcm, weights)\n",
    "    #print (\"Matrix multiplied by weights (in column) =\", pcm_mult)\n",
    "\n",
    "    #Sum the lines\n",
    "    weighted_sum = np.sum(pcm_mult, axis=1)\n",
    "    #print (\"Weighted sum (in line) =\", weighted_sum)\n",
    "\n",
    "    #Mean of (weighted sums divided by weights (priority))\n",
    "    lambda_max = np.average(np.divide(weighted_sum, weights))\n",
    "    #print(\"Lambda max =\", lambda_max)\n",
    "\n",
    "    #Count number of rows in pairwise comparison matrix\n",
    "    nb_rows = pcm.shape[0]\n",
    "    #print(\"Number of rows in the matrix =\", nb_rows)\n",
    "\n",
    "    #Consistency Index (Lambda max - nb rows)/(nb_rows-1)\n",
    "    ci = ((lambda_max-nb_rows)/(nb_rows-1))\n",
    "    #print(\"Consistency Index =\", ci)\n",
    "    \n",
    "    ##2.2-Random Index (RI)\n",
    "\n",
    "    #The RI values, as a function of the matrix size, are given in (Saaty et al., 2007)\n",
    "    ri = 0.0\n",
    "    if nb_rows == 3:\n",
    "        ri = 0.52\n",
    "    elif nb_rows == 4:\n",
    "        ri = 0.89\n",
    "    elif nb_rows == 5:\n",
    "        ri = 1.11\n",
    "    elif nb_rows == 6:\n",
    "        ri = 1.25\n",
    "    elif nb_rows == 7:\n",
    "        ri = 1.35\n",
    "    elif nb_rows == 8:\n",
    "        ri = 1.40\n",
    "    elif nb_rows == 9:\n",
    "        ri = 1.45\n",
    "    elif nb_rows == 10:\n",
    "        ri = 1.49\n",
    "    elif nb_rows == 11:\n",
    "        ri = 1.52\n",
    "    elif nb_rows == 12:\n",
    "        ri = 1.54\n",
    "    elif nb_rows == 13:\n",
    "        ri = 1.56\n",
    "    elif nb_rows == 14:\n",
    "        ri = 1.58\n",
    "    elif nb_rows == 15:\n",
    "        ri = 1.59\n",
    "    #print(\"Random Index (RI) =\", ri)\n",
    "\n",
    "    ##2.3-Consistency Ratio (CR)\n",
    "    cr = (ci/ri)\n",
    "    #print(\"Consistency Ratio (CR) =\",cr)\n",
    "    \n",
    "    #Create a csv file with the consistency index, the random index, and the consistency ratio\n",
    "    filename = 'consistency_%s' %(os.path.basename(file))\n",
    "    path_file = os.path.join(outdata['output_matrices_analysis'], filename)\n",
    "    ciricr = (ci,ri,cr)\n",
    "    np.savetxt(path_file, ciricr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute arithmetic mean of weights\n",
    "\n",
    "def readmycsv(args):\n",
    "    return pd.read_csv(args, header=None)\n",
    "\n",
    "##Create list of csv files\n",
    "factors = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'weight_vector_adults_factors*.csv'))\n",
    "\n",
    "#Concatenate csv files\n",
    "weights_concat = pd.concat(map(readmycsv, factors), axis = 1, ignore_index=True)\n",
    "#Compute arithmentic mean and export to csv file\n",
    "weights_mean = weights_concat.mean(axis=1)\n",
    "weights_mean.to_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_adults_factors_mean.csv'), header=None,index=False, float_format=\"%.4f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create summary CSVs with consistency index, random index, and consistency ratio\n",
    "#Ideally there should be row names and column names (to do when I have time)\n",
    "#Row 1:Consistency Index; Row 2:Random Index; Row 3:Consistency Ratio\n",
    "\n",
    "##Create lists\n",
    "land_cover = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'consistency_adults_lc*.csv'))\n",
    "land_use = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'consistency_adults_lu*.csv'))\n",
    "factors = glob.glob(os.path.join(outdata['output_matrices_analysis'], 'consistency_adults_factors*.csv'))\n",
    "\n",
    "#Concatenate land cover consistency\n",
    "consistency_concat = pd.concat(map(readmycsv, land_cover), axis = 1, ignore_index=True)\n",
    "consistency_concat.to_csv(os.path.join(outdata['output_matrices_analysis'],\"consistency_adults_concat_lc.csv\"), index=False, header=None, float_format=\"%.4f\")\n",
    "\n",
    "#Concatenate land use consistency\n",
    "#consistency_concat = pd.concat(map(readmycsv, land_use), axis = 1, ignore_index=True)\n",
    "consistency_concat.to_csv(os.path.join(outdata['output_matrices_analysis'], \"consistency_adults_concat_lu.csv\"), index=False, header=None, float_format=\"%.4f\")\n",
    "\n",
    "#Concatenate factors consistency\n",
    "#consistency_concat = pd.concat(map(readmycsv, factors), axis = 1, ignore_index=True)\n",
    "consistency_concat.to_csv(os.path.join(outdata['output_matrices_analysis'], \"consistency_adults_concat_factors.csv\"), index=False, header=None, float_format=\"%.4f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:purple\">STOP ! Please check the consistency ratio and evaluate if experts should revise their judgments.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press enter to continue \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input('Press enter to continue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating criteria to produce the adult Anopheles HSI map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_hsi=start_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region\n",
    "gscript.run_command('g.region', vector='AOI', align='DTM', res=5)\n",
    "\n",
    "#Mask areas outside the population map\n",
    "gscript.run_command('r.mask', overwrite=True, raster='POP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0\n",
      "0  0.1075\n",
      "1  0.1502\n",
      "2  0.4700\n",
      "3  0.2723\n",
      "ADULTS_SUITABILITY_max = 100.0\n",
      "ADULTS_SUITABILITY_min = 0.0\n",
      "ADULTS_SUITABILITY = round(((ADULTS_SUITABILITY - 0.0) / (100.0 - 0.0)*100))\n"
     ]
    }
   ],
   "source": [
    "#Breeding sites are defined as class \"Optimal\"\n",
    "#Use the factor weights determined by vector ecology experts\n",
    "#Calculate the weighted sum of factors to create the layer \"ADULTS_SUITABILITY\"\n",
    "\n",
    "df_adults_factors_weights = pd.read_csv(os.path.join(outdata['output_matrices_analysis'],'weight_vectors_adults_factors_mean.csv'), header=None)\n",
    "print(df_adults_factors_weights)\n",
    "w1 = df_adults_factors_weights[0].values[0]\n",
    "w2 = df_adults_factors_weights[0].values[1]\n",
    "w3 = df_adults_factors_weights[0].values[2]\n",
    "w4 = df_adults_factors_weights[0].values[3]\n",
    "\n",
    "formula = '%s = (%s * %s) + (%s * %s) + (%s * %s) + (%s * %s)' %('ADULTS_SUITABILITY',w1, 'A_LC', w2, 'A_LU', w3, 'A_DISPERSAL', w4, 'A_BU')\n",
    "gscript.mapcalc(formula, overwrite=True)\n",
    "\n",
    "\n",
    "#Scale the values to [0;100] \n",
    "rastinfo = gscript.raster_info('ADULTS_SUITABILITY')\n",
    "outmax = '%s_max' %('ADULTS_SUITABILITY')\n",
    "rastmax = rastinfo['max']\n",
    "print(outmax, '=', rastmax ) \n",
    "outmin = '%s_min' %('ADULTS_SUITABILITY')\n",
    "rastmin = rastinfo['min']\n",
    "print(outmin, '=', rastmin )\n",
    "\n",
    "formula = '%s = round(((%s - %s) / (%s - %s)*100))' %('ADULTS_SUITABILITY','ADULTS_SUITABILITY',rastmin,rastmax,rastmin)\n",
    "print(formula)\n",
    "gscript.mapcalc(formula, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HSI map produced in 7.6 seconds'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_hsi,\"HSI map produced in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling to 100m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "starttime_resample=start_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region (use POP geometry)\n",
    "gscript.run_command('g.region', raster='POP')\n",
    "\n",
    "#Mask areas outside the population map\n",
    "gscript.run_command('r.mask', overwrite=True, raster='POP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADULTS_HSI_AVG_100M_max = 87.3325\n",
      "ADULTS_HSI_AVG_100M_min = 0.0\n",
      "ADULTS_HSI_AVG_100M = round(((ADULTS_HSI_AVG_100M - 0.0) / (87.3325 - 0.0)*100))\n"
     ]
    }
   ],
   "source": [
    "#Resample HSI classes to 100 m (sum)\n",
    "gscript.run_command('r.resamp.stats', overwrite=True, \n",
    "                    input='ADULTS_SUITABILITY', method='average', output='ADULTS_HSI_AVG_100M')\n",
    "\n",
    "#Scale the values to [0;100] \n",
    "rastinfo = gscript.raster_info('ADULTS_HSI_AVG_100M')\n",
    "outmax = '%s_max' %('ADULTS_HSI_AVG_100M')\n",
    "rastmax = rastinfo['max']\n",
    "print(outmax, '=', rastmax ) \n",
    "outmin = '%s_min' %('ADULTS_HSI_AVG_100M')\n",
    "rastmin = rastinfo['min']\n",
    "print(outmin, '=', rastmin )\n",
    "\n",
    "formula = '%s = round(((%s - %s) / (%s - %s)*100))' %('ADULTS_HSI_AVG_100M','ADULTS_HSI_AVG_100M',rastmin,rastmax,rastmin)\n",
    "print(formula)\n",
    "gscript.mapcalc(formula, overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resampling done in 2.6 seconds'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print processing time\n",
    "print_processing_time(starttime_resample,\"Resampling done in \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning (e.g., 4 classes: Unsuitable, Marginal, Suitable and Optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADULTS_HS_CLASSES_100M = ADULTS_HS_CLASSES_100M*10\n"
     ]
    }
   ],
   "source": [
    "#Calculate percentiles\n",
    "gscript.run_command('r.quantile', overwrite=True, input='ADULTS_HSI_AVG_100M', percentiles= [60, 85, 95], flags='r', file = rules['recode_adultshsi2classes'])\n",
    "#Recode according to percentiles\n",
    "gscript.run_command('r.recode', overwrite=True, input= 'ADULTS_HSI_AVG_100M', output='ADULTS_HS_CLASSES_100M', rules= rules['recode_adultshsi2classes'])\n",
    "#Multiply by 10 (so the classes become 10, 20, 30, 40 and can be summed with the 3 population classes 1, 2, 3)\n",
    "#to produce a bivariate map\n",
    "formula = '%s = %s*10' %('ADULTS_HS_CLASSES_100M','ADULTS_HS_CLASSES_100M')\n",
    "print(formula)\n",
    "gscript.mapcalc(formula, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Export the result (hsi and hs classes at 100m) to tiff file, for making map layouts in QGIS\n",
    "#gscript.run_command('r.out.gdal', overwrite=True, flags = 'm', input='ADULTS_HSI_AVG_100M', output= os.path.join(outdata['outputdir'], 'dakar_adult_vectors_hsi_100m.tif'))\n",
    "gscript.run_command('r.out.gdal', overwrite=True, flags = 'm', input='ADULTS_HS_CLASSES_100M', output= os.path.join(outdata['outputdir'], 'dakar_adult_vectors_hs_classes_100m.tif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossing adult HS and population density to produce a simple malaria exposure map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set computational region (use POP geometry)\n",
    "gscript.run_command('g.region', raster='POP')\n",
    "\n",
    "#Mask areas outside the population map\n",
    "gscript.run_command('r.mask', overwrite=True, raster='POP')\n",
    "\n",
    "#Combine adult Anopheles HS and population density (addition)\n",
    "gscript.run_command('r.mapcalc', overwrite=True, \n",
    "                    expression='BIVARIATE_MALARIA_EXPOSURE = ADULTS_HS_CLASSES_100M + POP_LOG_CLASSES')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Export map to tiff file, for making map layout in QGIS\n",
    "gscript.run_command('r.out.gdal', overwrite=True, flags = 'm', input='BIVARIATE_MALARIA_EXPOSURE', output= os.path.join(outdata['outputdir'],'dakar_malaria_exposure_100m.tif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Processing completed!</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
